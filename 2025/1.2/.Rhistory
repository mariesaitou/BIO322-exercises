}
remDr$navigate(url)
Sys.sleep(10)  # Increase wait time for JavaScript to load the table
# Extract page source after JavaScript execution
page_source <- remDr$getPageSource()[[1]]
page_content <- read_html(page_source)
# Extract table with scientific names
tables <- page_content %>% html_nodes("table")
if (length(tables) > 0) {
for (i in seq_along(tables)) {
records <- tables[[i]] %>%
html_table(fill = TRUE) %>%
as.data.frame()
# Trim column names to remove extra spaces
colnames(records) <- trimws(colnames(records))
if ("Scientific Name" %in% colnames(records)) {
species <- records$`Scientific Name` %>% unique()
species_list <- unique(c(species_list, species))
}
}
} else {
print("No data found on page.")  # Debugging output
break  # Stop if no data is found on the current page
}
Sys.sleep(2)  # Pause between requests to avoid being blocked
}
library(RSelenium)
library(dplyr)
library(stringr)
library(readr)
# Start RSelenium server
rD <- rsDriver(browser = "firefox", port = 4444L, verbose = FALSE, check = FALSE)
remDr <- rD$client
# Define base URL for GGBN search results (modify as needed)
base_url <- "https://www.ggbn.org/ggbn_portal/search/result?sampletype=DNA&kingdom=Animalia&institution=NHMO%2C+Oslo&page="
# Initialize empty vector for species names
species_list <- c()
# Set maximum pages to scrape (adjust if necessary)
max_pages <- 25  # Change this based on actual pagination limits
for (page in 1:max_pages) {
url <- paste0(base_url, as.character(page))  # Ensure URL is a valid string
print(paste("Navigating to URL:", url))  # Debugging output
print(paste("URL Type:", class(url)))  # Debugging: Check data type
if (!is.character(url) || length(url) != 1) {
stop(paste("Error: URL is not a single valid string. Current URL:", url))
}
# Use executeScript instead of navigate
remDr$executeScript(paste("window.location.href='", url, "';"))
Sys.sleep(10)  # Increase wait time for JavaScript to load the table
# Print current URL to verify navigation
current_url <- remDr$getCurrentUrl()[[1]]
print(paste("Current URL in browser:", current_url))
# Extract page source after JavaScript execution
page_source <- remDr$getPageSource()[[1]]
page_content <- read_html(page_source)
# Extract table with scientific names
tables <- page_content %>% html_nodes("table")
if (length(tables) > 0) {
for (i in seq_along(tables)) {
records <- tables[[i]] %>%
html_table(fill = TRUE) %>%
as.data.frame()
# Trim column names to remove extra spaces
colnames(records) <- trimws(colnames(records))
if ("Scientific Name" %in% colnames(records)) {
species <- records$`Scientific Name` %>% unique()
species_list <- unique(c(species_list, species))
}
}
} else {
print("No data found on page.")  # Debugging output
break  # Stop if no data is found on the current page
}
Sys.sleep(2)  # Pause between requests to avoid being blocked
}
library(RSelenium)
library(dplyr)
library(stringr)
library(readr)
# Start RSelenium server
rD <- rsDriver(browser = "firefox", port = 4444L, verbose = FALSE, check = FALSE)
remDr <- rD$client
# Define base URL for GGBN search results (modify as needed)
base_url <- "https://www.ggbn.org/ggbn_portal/search/result?sampletype=DNA&kingdom=Animalia&institution=NHMO%2C+Oslo&page="
# Initialize empty vector for species names
species_list <- c()
# Set maximum pages to scrape (adjust if necessary)
max_pages <- 25  # Change this based on actual pagination limits
for (page in 1:max_pages) {
url <- paste0(base_url, as.character(page))  # Ensure URL is a valid string
print(paste("Navigating to URL:", url))  # Debugging output
print(paste("URL Type:", class(url)))  # Debugging: Check data type
if (!is.character(url) || length(url) != 1) {
stop(paste("Error: URL is not a single valid string. Current URL:", url))
}
# Print current URL before navigating
print(paste("Before navigation, current URL:", remDr$getCurrentUrl()[[1]]))
# Navigate to URL
tryCatch({
remDr$navigate(url)
Sys.sleep(15)  # Allow more time for page to load
}, error = function(e) {
print(paste("Navigation error:", e))
next  # Skip to next iteration if navigation fails
})
# Print current URL after navigation
print(paste("After navigation, current URL:", remDr$getCurrentUrl()[[1]]))
# Extract page source after JavaScript execution
page_source <- remDr$getPageSource()[[1]]
page_content <- read_html(page_source)
# Extract table with scientific names
tables <- page_content %>% html_nodes("table")
if (length(tables) > 0) {
for (i in seq_along(tables)) {
records <- tables[[i]] %>%
html_table(fill = TRUE) %>%
as.data.frame()
# Trim column names to remove extra spaces
colnames(records) <- trimws(colnames(records))
if ("Scientific Name" %in% colnames(records)) {
species <- records$`Scientific Name` %>% unique()
species_list <- unique(c(species_list, species))
}
}
} else {
print("No data found on page.")  # Debugging output
break  # Stop if no data is found on the current page
}
Sys.sleep(2)  # Pause between requests to avoid being blocked
}
library(RSelenium)
library(dplyr)
library(stringr)
library(readr)
# Function to start Selenium
start_selenium <- function() {
rD <- rsDriver(browser = "firefox", port = 4444L, verbose = FALSE, check = FALSE)
return(rD$client)
}
# Define base URL for GGBN search results (modify as needed)
base_url <- "https://www.ggbn.org/ggbn_portal/search/result?sampletype=DNA&kingdom=Animalia&institution=NHMO%2C+Oslo&page="
# Initialize empty vector for species names
species_list <- c()
# Set maximum pages to scrape (adjust if necessary)
max_pages <- 25  # Change this based on actual pagination limits
for (page in 1:max_pages) {
remDr <- start_selenium()  # Restart Selenium session each iteration
url <- paste0(base_url, as.character(page))  # Ensure URL is a valid string
print(paste("Navigating to URL:", url))  # Debugging output
tryCatch({
remDr$navigate(url)
Sys.sleep(15)  # Allow more time for page to load
}, error = function(e) {
print(paste("Navigation error:", e))
next  # Skip to next iteration if navigation fails
})
# Print current URL after navigation
print(paste("After navigation, current URL:", remDr$getCurrentUrl()[[1]]))
# Extract page source after JavaScript execution
page_source <- remDr$getPageSource()[[1]]
if (nchar(page_source) < 1000) {  # Check if valid HTML is returned
print("Warning: Page source seems too short, potential loading error.")
next
}
page_content <- read_html(page_source)
# Extract table with scientific names
tables <- page_content %>% html_nodes("table")
if (length(tables) > 0) {
for (i in seq_along(tables)) {
records <- tables[[i]] %>%
html_table(fill = TRUE) %>%
as.data.frame()
# Trim column names to remove extra spaces
colnames(records) <- trimws(colnames(records))
if ("Scientific Name" %in% colnames(records)) {
species <- records$`Scientific Name` %>% unique()
species_list <- unique(c(species_list, species))
}
}
} else {
print("No data found on page.")  # Debugging output
break  # Stop if no data is found on the current page
}
remDr$close()  # Close Selenium session after each iteration
Sys.sleep(2)  # Pause between requests to avoid being blocked
}
install.packages("webdriver")
webdriver::install_phantomjs()
install.packages("RSelenium")
install.packages("rvest")
library(RSelenium)
library(dplyr)
library(stringr)
library(readr)
# Function to start Selenium
start_selenium <- function() {
rD <- rsDriver(browser = "firefox", port = 4444L, verbose = FALSE, check = FALSE)
return(rD$client)
}
# Define base URL for GGBN search results (modify as needed)
base_url <- "https://www.ggbn.org/ggbn_portal/search/result?sampletype=DNA&kingdom=Animalia&institution=NHMO%2C+Oslo&page="
# Initialize empty vector for species names
species_list <- c()
# Set maximum pages to scrape (adjust if necessary)
max_pages <- 25  # Change this based on actual pagination limits
for (page in 1:max_pages) {
remDr <- start_selenium()  # Restart Selenium session each iteration
url <- paste0(base_url, as.character(page))  # Ensure URL is a valid string
print(paste("Navigating to URL:", url))  # Debugging output
tryCatch({
remDr$navigate(url)
Sys.sleep(15)  # Allow more time for page to load
}, error = function(e) {
print(paste("Navigation error:", e))
next  # Skip to next iteration if navigation fails
})
# Print current URL after navigation
print(paste("After navigation, current URL:", remDr$getCurrentUrl()[[1]]))
# Extract page source after JavaScript execution
page_source <- remDr$getPageSource()[[1]]
if (nchar(page_source) < 1000) {  # Check if valid HTML is returned
print("Warning: Page source seems too short, potential loading error.")
next
}
page_content <- read_html(page_source)
# Extract table with scientific names
tables <- page_content %>% html_nodes("table")
if (length(tables) > 0) {
for (i in seq_along(tables)) {
records <- tables[[i]] %>%
html_table(fill = TRUE) %>%
as.data.frame()
# Trim column names to remove extra spaces
colnames(records) <- trimws(colnames(records))
if ("Scientific Name" %in% colnames(records)) {
species <- records$`Scientific Name` %>% unique()
species_list <- unique(c(species_list, species))
}
}
} else {
print("No data found on page.")  # Debugging output
break  # Stop if no data is found on the current page
}
remDr$close()  # Close Selenium session after each iteration
Sys.sleep(2)  # Pause between requests to avoid being blocked
}
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)
library(ggmap)
api_key <- "kp67jn4hdfd0"
# ツグミ属 (*Turdus*) のファミリーコード（Turdidae）を取得
family_code <- "turdidae"
# eBird API エンドポイント
url <- paste0("https://api.ebird.org/v2/data/obs/geo/recent?lat=0&lng=0&dist=20000&familyCode=", family_code)
# APIリクエストの送信
response <- GET(url, add_headers("X-eBirdApiToken" = api_key))
# 結果をデータフレーム化
if (status_code(response) == 200) {
turdus_data <- fromJSON(content(response, "text"), flatten = TRUE)
turdus_df <- as.data.frame(turdus_data)
print("データ取得成功！")
print(head(turdus_df))  # 最初の数行を表示
} else {
print(paste("APIエラー: ", status_code(response)))
}
https://api.ebird.org/v2/ref/region/info/{{regionCode}}
# APIリクエストの送信
response <- GET(url, add_headers("X-eBirdApiToken" = api_key))
# eBirdの「ヨーロッパ（EU）」全体でツグミ科（Turdidae）の観察データを取得
region_code <- "EU"  # ヨーロッパ大陸
family_code <- "turdidae"  # ツグミ科
# APIリクエスト
url <- paste0("https://api.ebird.org/v2/data/obs/", region_code, "/recent?familyCode=", family_code)
response <- GET(url, add_headers("X-eBirdApiToken" = api_key))
# レスポンスを処理
if (status_code(response) == 200) {
bird_data <- fromJSON(content(response, "text"), flatten = TRUE)
turdus_df <- as.data.frame(bird_data) %>%
select(comName, sciName, obsDt, lat, lng, locName) %>%
rename(Species = comName, ScientificName = sciName, Date = obsDt, Latitude = lat, Longitude = lng, Location = locName)
print("データ取得成功！")
print(head(turdus_df))
} else {
print(paste("APIエラー:", status_code(response)))
}
# レスポンスを処理
if (status_code(response) == 200) {
bird_data <- fromJSON(content(response, "text"), flatten = TRUE)
turdus_df <- as.data.frame(bird_data)
print("データ取得成功！")
print(head(turdus_df))
} else {
print(paste("APIエラー:", status_code(response)))
}
}bird_data
bird_data
# 完全統合版 run_allmodels.R：全関数定義・自己完結・クラス不均衡対応付き
library(tidyverse)
library(caret)
library(xgboost)
library(pROC)
args <- commandArgs(trailingOnly = TRUE)
if (length(args) != 2) {
stop("Usage: Rscript run_allmodels.R <microbiome_file> <trait_name>")
}
install.packages(c("swirl", "swirlify"))
swirl()
library("swirl")
swirl()
1
BiocManager::install("ViSEAGO")
setwd("/Users/saitoumarie/Library/CloudStorage/Dropbox/Norway/class/Bio322_2025/BIO322-exercises/2025/1.2")
suppressPackageStartupMessages({
library(readr)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(ggrepel)
})
gff <- read_te_gff("Atha_nestOrigPairsFinal.gff")
suppressPackageStartupMessages({
library(readr)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(ggrepel)
})
#--- Read a GFF file and expand the attributes column
read_te_gff <- function(path) {
cols <- c("seqid","source","type","start","end","score","strand","phase","attributes")
gff <- readr::read_tsv(
path,
comment = "#",
col_names = cols,
col_types = cols(
seqid = col_character(),
source = col_character(),
type = col_character(),
start = col_integer(),
end   = col_integer(),
score = col_character(),
strand= col_character(),
phase = col_character(),
attributes = col_character()
)
)
# Expand attributes given as key=value pairs separated by ';'
attr_df <- gff$attributes %>%
str_split_fixed(";", n = 50) %>%   # a practical upper bound
as.data.frame(stringsAsFactors = FALSE)
parse_attr_cols <- function(x) {
kv <- str_split_fixed(x, "=", 2)
tibble(key = kv[,1], val = kv[,2]) %>%
filter(key != "", !is.na(val)) %>%
distinct(key, .keep_all = TRUE) %>% # keep first occurrence for duplicated keys
pivot_wider(names_from = key, values_from = val)
}
parsed <- lapply(seq_len(nrow(attr_df)), function(i) {
parse_attr_cols(as.character(unlist(attr_df[i, ])))
})
# Bind rows, missing keys become NA
parsed_attr <- bind_rows(parsed)
out <- bind_cols(gff %>% select(-attributes), parsed_attr) %>%
mutate(
ID       = coalesce(ID, name),
position = coalesce(position, ""),
role     = case_when(
position == "original" ~ "TE_original",
position == "nested"   ~ "TE_nested",
TRUE ~ NA_character_
)
)
out
}
gff <- read_te_gff("Atha_nestOrigPairsFinal.gff")
#--- Draw a schematic for a specified Original/Nested TE pair
plot_te_nesting <- function(gff_tbl, original_id, nested_id, pad_bp = 500) {
# Pick the first feature by start if multiple match the ID/name
ori <- gff_tbl %>% filter(ID == original_id | name == original_id) %>% slice_min(start, n = 1)
nes <- gff_tbl %>% filter(ID == nested_id   | name == nested_id)   %>% slice_min(start, n = 1)
if (nrow(ori) == 0) stop("Original ID not found: ", original_id)
if (nrow(nes) == 0) stop("Nested ID not found: ", nested_id)
if (ori$seqid[1] != nes$seqid[1]) {
warning("Original and Nested are on different contigs. The plot will still be drawn, but please verify the data.")
}
chr   <- ori$seqid[1]
x_min <- min(ori$start, nes$start) - pad_bp
x_max <- max(ori$end,   nes$end)   + pad_bp
# Layers
# Original fragmented segments
frag_ori <- gff_tbl %>%
filter(Parent %in% c(original_id), type == "repeat_fragment")
# Original LTRs
ltr_ori <- gff_tbl %>%
filter(Parent %in% c(original_id), type == "long_terminal_repeat")
# Nested body (rectangle)
body_nes <- tibble(
start = nes$start, end = nes$end, label = nested_id
)
# Nested LTRs
ltr_nes <- gff_tbl %>%
filter(Parent %in% c(nested_id), type == "long_terminal_repeat")
# Domains for Original/Nested
dom_ori <- gff_tbl %>%
filter(Parent %in% c(original_id), type == "polypeptide_conserved_region") %>%
mutate(label = coalesce(name, ""),
xlab = (start + end)/2,
ylab = 1.05)
dom_nes <- gff_tbl %>%
filter(Parent %in% c(nested_id), type == "polypeptide_conserved_region") %>%
mutate(label = coalesce(name, ""),
xlab = (start + end)/2,
ylab = 2.5)
# Track assignments on y-axis
# y=3: Original overall band (light gray) and fragments (dark gray)
# y=2: Nested body/LTRs/domains
# y=1: Original domains (separate row to avoid overlap)
base_ori <- tibble(start = ori$start, end = ori$end, y = 3, what = "Original")
# Note: a 'base_nes' object would be unused here and is intentionally omitted.
# Reproducible repel placement
set.seed(1234)
p <- ggplot() +
# Original overall background
geom_rect(
data = base_ori,
aes(xmin = start, xmax = end, ymin = 2.8, ymax = 3.2),
fill = "grey90", color = NA
) +
# Original fragments
geom_rect(
data = frag_ori,
aes(xmin = start, xmax = end, ymin = 2.85, ymax = 3.15),
fill = "grey40", color = "grey20"
) +
# Original LTRs
geom_rect(
data = ltr_ori,
aes(xmin = start, xmax = end, ymin = 2.8, ymax = 3.2),
fill = NA, color = "black", linetype = 2, linewidth = 0.4
) +
# Nested body
geom_rect(
data = body_nes,
aes(xmin = start, xmax = end, ymin = 1.8, ymax = 2.2),
fill = "#8FBCD4", color = "black"
) +
# Nested LTRs
geom_rect(
data = ltr_nes,
aes(xmin = start, xmax = end, ymin = 1.8, ymax = 2.2),
fill = NA, color = "black", linetype = 2, linewidth = 0.4
) +
# Nested domains (upper row)
geom_rect(
data = dom_nes,
aes(xmin = start, xmax = end, ymin = 2.25, ymax = 2.45),
fill = "#4C9ACB", color = "black"
) +
ggrepel::geom_text_repel(
data = dom_nes,
aes(x = xlab, y = ylab, label = label),
direction = "x",              # avoid overlaps horizontally, keep y fixed
min.segment.length = 0,
box.padding = 0.2,
point.padding = 0.1,
max.overlaps = Inf,
size = 3
) +
# Original domains (lower row)
geom_rect(
data = dom_ori,
aes(xmin = start, xmax = end, ymin = 0.8, ymax = 1.0),
fill = "#BDBDBD", color = "black"
) +
ggrepel::geom_text_repel(
data = dom_ori,
aes(x = xlab, y = ylab, label = label),
direction = "x",
min.segment.length = 0,
box.padding = 0.2,
point.padding = 0.1,
max.overlaps = Inf,
size = 3
) +
coord_cartesian(xlim = c(x_min, x_max), ylim = c(0.6, 3.4), expand = FALSE, clip = "off") +
labs(
title = paste0(chr, ": Original=", original_id, "  Nested=", nested_id),
x = "Genomic coordinate (bp)",
y = NULL
) +
theme_minimal(base_size = 11) +
theme(
panel.grid.major.y = element_blank(),
panel.grid.minor   = element_blank(),
axis.text.y        = element_blank(),
plot.title         = element_text(face = "bold"),
plot.margin        = margin(t = 10, r = 30, b = 10, l = 30)
)
return(p)
}
#--- Helper to save the plot
save_te_plot <- function(p, file = "te_nesting_schematic.pdf", width = 10, height = 3.5) {
ggsave(file, p, width = width, height = height, units = "in")
}
# Example usage
p <- plot_te_nesting(gff, original_id = "TE_BASE_375", nested_id = "TE_BASE_106")
print(p)
save_te_plot(p, "Atha_TE375_with_TE106.pdf")
